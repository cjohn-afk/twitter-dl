#! /usr/bin/env python3

from time import sleep
from pathlib import Path
import sys
#from datetime import datetime
from dateutil import parser
import pytz
import threading
import requests
from requests.structures import CaseInsensitiveDict
from sqlmanager import SQLManager
from queue import Queue
import db_queries
from argument_handler import args
from config import set_bearer_token, bearer_token

if args.bearer_token is None:
    print("No bearer token specified.")
else:
    set_bearer_token(args.bearer_token)
    
print("Using bearer token: " + bearer_token)

total_requests = 0
headers = CaseInsensitiveDict()
headers["Authorization"] = "Bearer " + bearer_token

def reformat_date(twitter_date):
    return parser.parse(twitter_date)
    #return  datetime.strptime(twitter_date,'%a %b %d %H:%M:%S +0000 %Y').replace(tzinfo=pytz.UTC)

def request_trimmer():
    global total_requests
    while True:
        if total_requests > 0:
            total_requests -= 1
        sleep(1.15)

def request_limiter():
    global total_requests
    while True:
        if total_requests < 20:
            break
        sleep(1.5)

def download(url, location): 
    filename = Path(location + url.split('/')[-1].split('?')[0])
    if not filename.exists():
        print("Downloading " + url + " to " + str(filename) + ".")
        data = requests.get(url).content
        with open(filename, "wb") as file:
            file.write(data)
    else:
        print(url)
        print(str(filename) + " already exists. It will not be downloaded.")
        
def create_directory(username):
    Path(username).mkdir()
    print("Created directory './" + username + "'.")

def read_user_list(user_list_file):
    user_list = []
    with open(user_list_file, 'rt') as file:
        while(user := file.readline().rstrip()):
            if user is not None:
                user_list.append(user)
            
    return user_list

def download_user_media(username):
    global total_requests
    pagination_token = None
    
    print("Downloading media from " + username)
    
    account = db_queries.get_account_by_username(username)
    if account is None:
        print("No stored ID for " + username + ". Requesting ID...")
        response = requests.get('https://api.twitter.com/2/users/by/username/' + username, headers=headers)
        total_requests += 1
        request_limiter()
        #TODO: Handle case where username doesn't exist or is not found. Currently the assignment of 'id' below causes an error in this case because the response will not contain a data key if the username is not found.
        id = int(response.json()['data']['id'])
        db_queries.add_account(id, username)
        account_id = db_queries.get_account_by_user_id(id).id
    else:
        id = account.user_id
        account_id = account.id

    if not Path(username).exists():
        directory_exists = False
    else:
        if Path(username).is_dir():
            directory_exists = True
        else:
            pass
            ### TODO HANDLE CASE WHERE A FILE EXISTS AS THE USERNAME

    user_directory = "./" + username + "/"

    most_recent_post_date = db_queries.get_latest_post_date(db_queries.get_account_by_user_id(id).id)

    pagination_token = -1
    first_request = True
    while pagination_token is not None:
        query = {'exclude':'retweets,replies', 'expansions':'attachments.media_keys', 'max_results':100, 'tweet.fields':'created_at,author_id'}
        if pagination_token != -1 and pagination_token is not None:
            query["pagination_token"] = pagination_token

        response = requests.get('https://api.twitter.com/2/users/'+str(id)+'/tweets', params=query, headers=headers)
        total_requests += 1
        request_limiter()
        
        try:
            response.json()['data']
        except:
            break
        else:
            if first_request:
                newest_tweet = response.json()['data'][0]
                created_at = reformat_date(newest_tweet['created_at'])
                db_queries.add_post(account_id, newest_tweet['id'], created_at)
                first_request = False
            
        try:
            response.json()['meta']['next_token']
        except:
            pagination_token = None
        else:
            pagination_token = response.json()['meta']['next_token']
        
        id_list = []
        ## TODO: Rework the check for newest tweet vs tweet.
        try:
            for tweet in response.json()['data']:
                created_at = reformat_date(tweet['created_at'])
                if most_recent_post_date is not None and created_at <= pytz.UTC.localize(most_recent_post_date):
                    raise Exception()
                try:
                    tweet['attachments']
                except:
                    pass
                else:
                    id_list.append(tweet['id'])
        except:
            pagination_token = None
            pass

        for post_id in id_list:
            query = {'id':post_id, 'include_entities':'true', 'trim_user':'true','include_ext_alt_text':'false','include_my_retweet':'false','include_card_uri':'false'}
            response = requests.get('https://api.twitter.com/1.1/statuses/show.json', headers=headers, params=query)
            created_at = reformat_date(response.json()['created_at'])
            total_requests += 1
            request_limiter()
            
            media = None
            try:
                media = response.json()['extended_entities']['media']
            except:
                print("No media in post.")
            else:
                if not directory_exists:
                    create_directory(username)
                    directory_exists = True
                for m in media:
                    
                    if m['type'] == 'video':
                        videos = m['video_info']['variants']
                        bitrate = 0
                        video_url = None
                        for v in videos:
                            if v['content_type'] == 'video/mp4':
                                if v['bitrate'] > bitrate:
                                    bitrate = v['bitrate']
                                    video_url = v['url']
                        download(video_url, user_directory)
                    else:
                        download(m['media_url_https']+"?name=orig", user_directory)
                        
                    db_queries.add_post(account_id, response.json()['id'], created_at)
                        
        if pagination_token is None:
            print("End of downloadable media from " + username + "\n")

def main(args):

    if args.list is not None:
        print("Reading usernames from " + args.list)
        user_list = read_user_list(Path(args.list).expanduser())
    if args.username is not None and args.username:
        user_list.append(args.username)
    #TODO: Handle case where no userlist or usernames are specified.
        
    print(user_list)
    
    user_queues = {}
    sql_manager_queue = Queue()
    
    for username in user_list:
        user_queues[username] = Queue()

    sql_manager = SQLManager(sql_manager_queue, user_queues)
    sql_manager_thread = threading.Thread(target=sql_manager.run)
    
    request_trimmer_thread = threading.Thread(target=request_trimmer)
    request_trimmer_thread.daemon = True
    request_trimmer_thread.start()

    for username in user_list:
        #user_downloader_thread = threading.Thread(target=download_user_media, args=(username,user_queues[username],))
        #user_downloader_thread.start()
        download_user_media(username)
        
    
    total_requests = -1

main(args)
sys.exit()
